# find_similar_principles.py

import numpy as np
import faiss
import json
import os
from tqdm import tqdm
import pickle

def load_principles_with_embeddings(filepath: str):
    """
    åŠ è½½åŸåˆ™åˆ—è¡¨ï¼Œå‡è®¾æ ¼å¼ï¼š
    [
      {
        "principle": "You should be helpful.",
        "embedding": [0.1, 0.2, ..., 0.768]
      },
      ...
    ]
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        # data = json.load(f)
        data = [json.loads(line) for line in f]
    print(f"âœ… åŠ è½½ {len(data)} æ¡åŸåˆ™")
    return data

def build_faiss_index(embeddings: np.ndarray, index_type="HNSW"):
    """
    æ„å»º Faiss ç´¢å¼•
    :param embeddings: np.array of shape (N, D)
    :param index_type: "HNSW" or "IVFPQ"
    :return: faiss index
    """
    d = embeddings.shape[1]
    if index_type == "HNSW":
        # HNSW ç´¢å¼•ï¼ˆé€‚åˆé«˜ç²¾åº¦è¿‘ä¼¼æœ€è¿‘é‚»ï¼‰
        index = faiss.IndexHNSWFlat(d, 32)  # M=32
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 50
    elif index_type == "IVFPQ":
        # IVFPQ ç´¢å¼•ï¼ˆé€‚åˆè¶…å¤§è§„æ¨¡ï¼‰
        nlist = min(1000, len(embeddings) // 100)
        quantizer = faiss.IndexFlatIP(d)
        index = faiss.IndexIVFPQ(quantizer, d, nlist, 8, 8)  # 8-bit PQ
        index.train(embeddings)
    else:
        raise ValueError("Unsupported index type")

    # å½’ä¸€åŒ–å‘é‡ï¼ˆå› ä¸ºæˆ‘ä»¬è¦ç”¨å†…ç§¯åšç›¸ä¼¼åº¦ï¼‰
    faiss.normalize_L2(embeddings)
    index.add(embeddings)
    print(f"âœ… æ„å»º Faiss ç´¢å¼•å®Œæˆï¼Œç±»å‹: {index_type}")
    return index

def find_similar_pairs(principles, embeddings, thresholds=[0.98, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65], top_k=5000):
    """
    æŸ¥æ‰¾æ‰€æœ‰ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„ pair
    ä½¿ç”¨ Faiss è¿›è¡Œè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢
    """
    n = len(principles)
    d = embeddings.shape[1]

    # æ„å»ºç´¢å¼•
    index = build_faiss_index(embeddings, index_type="HNSW")

    # å­˜å‚¨ç»“æœ
    similar_pairs_by_threshold = {t: [] for t in thresholds}
    total_checked = 0

    # é€ä¸ªæŸ¥è¯¢
    for i in tqdm(range(n), desc="ğŸ” æŸ¥è¯¢ç›¸ä¼¼å¯¹", unit="principle"):
        query_vec = embeddings[i].reshape(1, -1)

        # æœç´¢ top_k æœ€è¿‘é‚»ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
        distances, indices = index.search(query_vec, top_k)

        # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆå†…ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå› ä¸ºå·²å½’ä¸€åŒ–ï¼‰
        similarities = 1 - distances  # å› ä¸º Faiss è¿”å›çš„æ˜¯è·ç¦»ï¼ˆ1 - cos_simï¼‰

        # éå†æ¯ä¸ªé‚»å±…
        for j, sim in zip(indices[0], similarities[0]):
            if i == j:  # è·³è¿‡è‡ªå·±
                continue
            if sim < 0.3:  # æå‰å‰ªæï¼Œå‡å°‘è®¡ç®—
                break
            total_checked += 1

            # åªè®°å½• i < j çš„ pairï¼Œé¿å…é‡å¤
            if i < j:
                for th in thresholds:
                    if sim >= th:
                        similar_pairs_by_threshold[th].append({
                            'idx1': int(i),
                            'idx2': int(j),
                            'similarity': float(sim),
                            'principle1': principles[i]['prompt'],
                            'principle2': principles[j]['prompt']
                        })

    print(f"âœ… æ€»å…±æ£€æŸ¥äº† {total_checked} å¯¹")
    return similar_pairs_by_threshold

def save_results(similar_pairs_by_threshold, output_dir="similar_pairs_30"):
    """
    ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    """
    os.makedirs(output_dir, exist_ok=True)

    for th, pairs in similar_pairs_by_threshold.items():
        filename = f"similar_pairs_{int(th*100)}.json"
        filepath = os.path.join(output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(pairs, f, indent=2, ensure_ascii=False)
        print(f"âœ… ä¿å­˜ {len(pairs)} å¯¹ç›¸ä¼¼åº¦ â‰¥{int(th*100)}% çš„åŸåˆ™åˆ°: {filepath}")

    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    report = {
        "thresholds": {},
        "total_principles": len(similar_pairs_by_threshold[list(similar_pairs_by_threshold.keys())[0]][0]['principle1']) if similar_pairs_by_threshold else 0
    }
    for th, pairs in similar_pairs_by_threshold.items():
        report["thresholds"][str(th)] = {
            "count": len(pairs),
            "filename": f"similar_pairs_{int(th*100)}.json"
        }

    with open(os.path.join(output_dir, "summary.json"), 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜è‡³: {os.path.join(output_dir, 'summary.json')}")

# ===========================
# ğŸš€ ä¸»å‡½æ•°
# ===========================

def main():
    print("ğŸš€ å¼€å§‹æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„åŸåˆ™å¯¹...")

    # Step 1: åŠ è½½æ•°æ®
    input_file = "/mnt/oss_data/llm_safety/datasets/value_principles-embedding.jsonl"  # æ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ–‡ä»¶è·¯å¾„
    if not os.path.exists(input_file):
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨ç”Ÿæˆç¤ºä¾‹æ•°æ®...")

    principles = load_principles_with_embeddings(input_file)

    # Step 2: æå– embedding
    embeddings = np.array([p['raw_output'] for p in principles], dtype=np.float32)
    print(f"ğŸ”¢ embedding ç»´åº¦: {embeddings.shape[1]}")

    # Step 3: æŸ¥æ‰¾ç›¸ä¼¼å¯¹
    thresholds = [0.98, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.30]
    similar_pairs = find_similar_pairs(principles, embeddings, thresholds=thresholds, top_k=5000)

    # Step 4: ä¿å­˜ç»“æœ
    save_results(similar_pairs)

    # Step 5: è¾“å‡ºç»Ÿè®¡
    print("\nğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
    for th, pairs in similar_pairs.items():
        print(f"  â‰¥{int(th*100)}%: {len(pairs):,} å¯¹")

    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()
